# evaluation/run_eval.py
# è·‘å®šæ—¶ä»»åŠ¡ï¼Œå®¡è®¡å†å² Trace
import time
import json
from ..config import langfuse
from ..judges import DeepSeekJudge

judge = DeepSeekJudge()

def run_auto_evaluation(batch_size=5):
    print(f"ğŸ” æ­£åœ¨ä» Langfuse API è·å–æœ€è¿‘çš„ {batch_size} æ¡ Trace...")

    try:
        # 1. è·å– Trace åˆ—è¡¨
        traces_response = langfuse.api.trace.list(limit=batch_size)
        traces = traces_response.data
    except Exception as e:
        print(f"âŒ è·å– Trace åˆ—è¡¨å¤±è´¥: {e}")
        return

    success_count = 0
    for trace in traces:
        t_id = trace.id
        u_input = getattr(trace, 'input', None)
        a_output = getattr(trace, 'output', None)

        if not u_input or not a_output:
            continue

        print(f"ğŸ“ æ­£åœ¨åˆ†æ Trace: {t_id[:8]}...")
        
        raw_context = None
        
        # ğŸŸ¢ [å…³é”®ä¿®æ”¹] æŒ–æ˜ä¸Šä¸‹æ–‡
        try:
            # è·å–å®Œæ•´è¯¦æƒ…
            try:
                full_trace = langfuse.api.trace.get(t_id)
            except AttributeError:
                full_trace = langfuse.api.traces.get(t_id)
            
            if hasattr(full_trace, 'observations'):
                for obs in full_trace.observations:
                    # 1. é”å®šå·¥å…·
                    if obs.name == "lookup_policy_doc" and obs.output:
                        output_data = obs.output
                        
                        # ğŸŸ¢ [å…¼å®¹æ€§ä¿®å¤] æ—¢æ”¯æŒå­—å…¸ï¼Œä¹Ÿæ”¯æŒå­—ç¬¦ä¸²
                        try:
                            # æƒ…å†µ A: SDK å·²ç»è‡ªåŠ¨è½¬æˆäº†å­—å…¸ (ä½ æ—¥å¿—é‡Œæ˜¾ç¤ºçš„æƒ…å†µ)
                            if isinstance(output_data, dict):
                                if "content" in output_data:
                                    raw_context = output_data["content"]
                                    print(f"   âœ… [Dict] æˆåŠŸæå–å‚è€ƒæ–‡æ¡£: {len(raw_context)} å­—ç¬¦")
                                    break
                            
                            # æƒ…å†µ B: ä¾ç„¶æ˜¯ JSON å­—ç¬¦ä¸² (æ—§æ•°æ®æˆ–ç‰¹å®šç¯å¢ƒ)
                            elif isinstance(output_data, str):
                                clean_str = output_data.strip().strip("`").replace("json", "")
                                temp_json = json.loads(clean_str)
                                if isinstance(temp_json, dict) and "content" in temp_json:
                                    raw_context = temp_json["content"]
                                    print(f"   âœ… [Str] æˆåŠŸæå–å‚è€ƒæ–‡æ¡£: {len(raw_context)} å­—ç¬¦")
                                    break
                        except Exception as e:
                            print(f"   âš ï¸ è§£æå·¥å…·è¾“å‡ºå¼‚å¸¸: {e}")
                            
        except Exception as e:
            print(f"   âš ï¸ è·å– Trace è¯¦æƒ…å¤±è´¥: {e}")

        # ğŸŸ¢ [åˆ†æ”¯è¯„åˆ†]
        if raw_context:
            # æœ‰ä¸Šä¸‹æ–‡ï¼šæŸ¥å¹»è§‰
            try:
                # ç¡®ä¿ä½ çš„ DeepSeekJudge ç±»é‡Œæœ‰è¿™ä¸ªæ–¹æ³•ï¼Œæ²¡æœ‰çš„è¯ä¼šè‡ªåŠ¨è·³åˆ° except
                eval_res = judge.evaluate_groundedness(str(u_input), str(a_output), str(raw_context))
                score_name = "faithfulness" # å¿ å®åº¦
                print(f"   ğŸ¤– æ­£åœ¨è¿›è¡Œå¹»è§‰å®¡è®¡ (Faithfulness)...")
            except AttributeError:
                print("   âš ï¸ Judge ç¼ºå°‘ evaluate_groundedness æ–¹æ³•ï¼Œé™çº§ä¸ºæ™®é€šè¯„åˆ†")
                eval_res = judge.evaluate(str(u_input), str(a_output))
                score_name = "deepseek_quality"
        else:
            # æ— ä¸Šä¸‹æ–‡ï¼šæŸ¥è´¨é‡
            print("   âš ï¸ æœªæ‰¾åˆ°ä¸Šä¸‹æ–‡ï¼Œè¿›è¡Œæ™®é€šè¯„åˆ† (Quality)...")
            eval_res = judge.evaluate(str(u_input), str(a_output))
            score_name = "deepseek_quality"

        # å›ä¼ åˆ†æ•°
        langfuse.create_score(
            trace_id=t_id,
            name=score_name,
            value=eval_res["score"],
            comment=eval_res["reasoning"]
        )
        
        success_count += 1
        time.sleep(0.5)

    langfuse.flush()
    print(f"âœ… è¯„æµ‹ç»“æŸï¼æˆåŠŸå¤„ç† {success_count} æ¡æ•°æ®ã€‚")

if __name__ == "__main__":
    run_auto_evaluation()