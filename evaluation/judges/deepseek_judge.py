# evaluation/judges/deepseek_judge.py
import json
from evaluation.config import deepseek_client, MODEL_DEEPSEEK_CHAT

class DeepSeekJudge:
    def __init__(self):
        self.client = deepseek_client
        self.model = MODEL_DEEPSEEK_CHAT

    def _call_llm(self, system_prompt, user_content):
        """é€šç”¨ LLM è°ƒç”¨æ–¹æ³•ï¼Œå¤„ç† JSON è¿”å›"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                # å¼ºåˆ¶è¦æ±‚ JSON æ ¼å¼è¿”å›ï¼ŒDeepSeek æ”¯æŒæ­¤å‚æ•°
                response_format={"type": "json_object"}, 
                temperature=0.1 # è¯„æµ‹ä»»åŠ¡éœ€è¦ä½éšæœºæ€§
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"âŒ è¯„æµ‹ LLM è°ƒç”¨å¤±è´¥: {e}")
            # å…œåº•è¿”å›ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
            return {"score": 0, "reasoning": f"System Error: {str(e)}"}

    def evaluate(self, question, answer):
        """æ™®é€šè¯„åˆ†ï¼šåªçœ‹å›ç­”æ˜¯å¦æµç•…ã€ç›¸å…³"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ AI è´¨é‡è¯„åˆ†å‘˜ã€‚
è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œ AI çš„å›ç­”ï¼Œä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. ç›¸å…³æ€§ï¼šæ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
2. å‡†ç¡®æ€§ï¼šé€»è¾‘æ˜¯å¦é€šé¡ºï¼Ÿ
3. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°ï¼Ÿ

è¯·è¿”å› JSON æ ¼å¼ï¼š
{
    "score": <int>,
    "reasoning": "<ç®€çŸ­çš„è¯„åˆ†ç†ç”±>"
}
"""
        user_content = f"ã€ç”¨æˆ·é—®é¢˜ã€‘: {question}\nã€AIå›ç­”ã€‘: {answer}"
        return self._call_llm(system_prompt, user_content)

    def evaluate_groundedness(self, question, answer, raw_context):
        """
        ğŸŸ¢ [æ–°å¢] å¹»è§‰å®¡è®¡ï¼šæ ¸å¿ƒæ˜¯å¯¹æ¯”â€œå‚è€ƒæ–‡æ¡£â€å’Œâ€œå›ç­”â€
        åªæœ‰å½“å›ç­”å®Œå…¨åŸºäºå‚è€ƒæ–‡æ¡£æ—¶ï¼Œæ‰èƒ½å¾—é«˜åˆ†ã€‚
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæå…¶ä¸¥è‹›çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å®¡è®¡å‘˜ã€‚
ä½ çš„ä»»åŠ¡æ˜¯éªŒè¯ AI çš„å›ç­”æ˜¯å¦**ä¸¥æ ¼å¿ å®äº**å‚è€ƒæ–‡æ¡£ã€‚

ã€è¯„åˆ†æ ‡å‡†ã€‘
- **10åˆ†**ï¼šå›ç­”çš„æ‰€æœ‰äº‹å®éƒ½åœ¨å‚è€ƒæ–‡æ¡£ä¸­æœ‰æ®å¯æŸ¥ï¼Œä¸”æ²¡æœ‰é—æ¼å…³é”®ä¿¡æ¯ã€‚
- **6-8åˆ†**ï¼šå›ç­”åŸºæœ¬æ­£ç¡®ï¼Œä½†åŒ…å«äº†ä¸€äº›æ–‡æ¡£ä¸­æœªæåŠçš„â€œå¸¸è¯†æ€§â€åºŸè¯ï¼Œæˆ–è€…é—æ¼äº†éƒ¨åˆ†ç»†èŠ‚ã€‚
- **0-5åˆ†**ï¼šã€ä¸¥é‡å¹»è§‰ã€‘å›ç­”ä¸­åŒ…å«äº†æ–‡æ¡£ä¸­å®Œå…¨æ²¡æœ‰çš„æ•°å­—ã€æ—¥æœŸã€æ¡æ¬¾ï¼Œæˆ–è€…å›ç­”ä¸æ–‡æ¡£äº‹å®å†²çªã€‚

ã€æ³¨æ„ã€‘
å³ä½¿ AI å›ç­”çš„æ˜¯ç°å®ä¸–ç•Œä¸­çš„çœŸç†ï¼ˆä¾‹å¦‚â€œå¤ªé˜³ä»ä¸œè¾¹å‡èµ·â€ï¼‰ï¼Œåªè¦å‚è€ƒæ–‡æ¡£é‡Œæ²¡å†™ï¼Œå°±å¿…é¡»è§†ä¸ºâ€œå¹»è§‰â€ï¼Œå¹¶æ‰£åˆ†ï¼å› ä¸ºæˆ‘ä»¬æµ‹è¯•çš„æ˜¯æ£€ç´¢èƒ½åŠ›ã€‚

è¯·è¿”å› JSON æ ¼å¼ï¼š
{
    "score": <int>,
    "reasoning": "<è¯¦ç»†æŒ‡å‡ºå“ªå¥è¯åœ¨æ–‡æ¡£é‡Œæ‰¾åˆ°äº†ï¼Œå“ªå¥è¯æ²¡æ‰¾åˆ°>"
}
"""
        # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢ Token æº¢å‡º
        truncated_context = raw_context[:8000] if len(raw_context) > 8000 else raw_context
        
        user_content = f"""
ã€å‚è€ƒæ–‡æ¡£ç‰‡æ®µã€‘:
{truncated_context}

ã€ç”¨æˆ·é—®é¢˜ã€‘: {question}

ã€AIå›ç­”ã€‘: {answer}
"""
        return self._call_llm(system_prompt, user_content)