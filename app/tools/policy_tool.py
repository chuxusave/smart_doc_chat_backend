# app/tools/policy_tool.py
# æ–‡æ¡£æ£€ç´¢å·¥å…·
from langchain.tools import tool
from llama_index.core.vector_stores.types import VectorStoreQueryMode
from app.services.rag_engine import get_index
from app.services.llm_factory import ModelFactory
import os
import json
# å°è£… Tools (å·¥å…·):LangChain çš„ @tool è£…é¥°å™¨éå¸¸å…³é”®ï¼Œå®ƒä¼šè‡ªåŠ¨æŠŠå‡½æ•°çš„ docstringï¼ˆæ³¨é‡Šï¼‰å˜æˆ Prompt å‘ç»™å¤§æ¨¡å‹ï¼Œæ‰€ä»¥æ³¨é‡Šå¿…é¡»å†™å¾—å¾ˆæ¸…æ¥šï¼
@tool
async def lookup_policy_doc(query: str) -> str:
    """
    ã€æŸ¥æ–‡æ¡£å·¥å…·ã€‘
     å½“ç”¨æˆ·è¯¢é—®å…¬å¸çš„è§„ç« åˆ¶åº¦ã€åˆåŒç»†èŠ‚ã€é¡¹ç›®å†…å®¹ã€è¯·å‡æµç¨‹ç­‰éç»“æ„åŒ–æ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ä½¿ç”¨æ­¤å·¥å…·ã€‚
     è¾“å…¥ï¼šå…·ä½“çš„æŸ¥è¯¢é—®é¢˜ï¼ˆä¾‹å¦‚ï¼š"CG2023åˆåŒçš„é‡‘é¢æ˜¯å¤šå°‘"ï¼‰ã€‚
    """
    try:
        # 1. è·å–èµ„æº (æŒ‰éœ€è·å–ï¼Œä¸å†æ˜¯å…¨å±€å˜é‡)
        index = get_index()  # åˆå§‹åŒ–ç´¢å¼•
        reranker = ModelFactory.get_reranker()


        # 2. æ··åˆæ£€ç´¢é€»è¾‘
        retriever = index.as_retriever(
            similarity_top_k=10, # å…ˆå¤šå–ä¸€ç‚¹
            vector_store_query_mode=VectorStoreQueryMode.HYBRID, # æ··åˆæ£€ç´¢
            alpha=0.5
        )
        nodes = await retriever.aretrieve(query)
        print(f"   æ£€ç´¢åˆ° {len(nodes)} ä¸ªæ–‡æ¡£ã€‚")
        
        # 3. é‡æ’åº
        filtered_nodes = reranker.postprocess_nodes(nodes, query_str=query)

        # åˆ†æ•°æˆªæ–­é€»è¾‘
        # é˜ˆå€¼è®¾å®šå»ºè®®ï¼š
        # BGE-Reranker çš„åˆ†æ•°ä¸æ˜¯ 0-1ï¼Œè€Œæ˜¯ logit å€¼ï¼ˆå¯èƒ½æ˜¯è´Ÿæ•°ï¼‰ã€‚
        # > 0 é€šå¸¸è¡¨ç¤ºç›¸å…³ã€‚
        # < -2 é€šå¸¸è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ã€‚
        # å»ºè®®å…ˆè®¾ä¸º -1.0 æˆ– 0.0 è¿›è¡Œæµ‹è¯•ã€‚å¦‚æœä½ å¸Œæœ›å®ƒæ›´ä¸¥è°¨ï¼Œè®¾é«˜ä¸€ç‚¹ï¼ˆå¦‚ 0.5ï¼‰ã€‚
        SCORE_THRESHOLD = 0.0
        valid_nodes = []
        sources_info = [] # ğŸŸ¢ ç”¨äºå­˜å‚¨å…ƒæ•°æ®
        for n in filtered_nodes:
            # æ‰“å°åˆ†æ•°æ–¹ä¾¿è°ƒè¯•
            print(f"   Ref doc: {n.metadata.get('file_name')} | Score: {n.score}")
            if n.score is not None and n.score > SCORE_THRESHOLD:
                valid_nodes.append(n)
                # ğŸŸ¢ æå–å…ƒæ•°æ®
                metadata = n.metadata if n.metadata else {}
                file_name = metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                # ç®€åŒ–æ–‡ä»¶åï¼Œåªä¿ç•™ basename
                base_name = os.path.basename(file_name)
                
                sources_info.append({
                    "file": base_name,
                    "page": metadata.get('page_label', '-'),
                    "score": f"{n.score:.2f}"
                })
        if not valid_nodes:
            print(f"ğŸ›‘ [RAG Tool] æ‰€æœ‰æ–‡æ¡£å¾—åˆ†å‡ä½äº {SCORE_THRESHOLD}ï¼Œè¿”å›æœªæ‰¾åˆ°ã€‚")
            return "ç³»ç»Ÿæç¤ºï¼šçŸ¥è¯†åº“ä¸­ã€æ²¡æœ‰æ‰¾åˆ°ã€‘åŒ…å«è¯¥é—®é¢˜ç­”æ¡ˆçš„æ–‡æ¡£ã€‚è¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ã€‚"        
        
        # 4. ç»“æœç»„è£…
        context_str = "\n\n".join([n.text for n in valid_nodes])
        # content å­—æ®µç»™ LLM é˜…è¯»ï¼Œsources å­—æ®µæˆ‘ä»¬å°†åœ¨ Router å±‚æ‹¦æˆª
        output_data = {
            "content": f"ã€å‚è€ƒæ–‡æ¡£ã€‘ï¼š\n{context_str}",
            "sources": sources_info
        }
        # è¿™é‡Œè¿”å›çš„å†…å®¹æ˜¯ç»™å¤§æ¨¡å‹çœ‹çš„ï¼Œå¯ä»¥åŠ ä¸€ç‚¹æç¤º
        # return f"ã€æŸ¥åˆ°çš„å‚è€ƒæ–‡æ¡£ã€‘ï¼š\n{context_str}"
        return json.dumps(output_data, ensure_ascii=False)
        
    except Exception as e:
        return f"æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"
    