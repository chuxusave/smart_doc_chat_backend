# app/services/rag_engine.py
import qdrant_client
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import VectorStoreIndex, StorageContext
from app.core.config import get_settings
from app.services.llm_factory import ModelFactory
from functools import lru_cache # ğŸ‘ˆ å¯¼å…¥ç¼“å­˜è£…é¥°å™¨

settings = get_settings()
@lru_cache() # ğŸ‘ˆ åŠ ä¸Šè¿™ä¸ªè£…é¥°å™¨ï¼Œç¡®ä¿å…¨å±€åªåˆå§‹åŒ–ä¸€æ¬¡ Index å’Œ è¿æ¥
def get_index():
    """è·å–å…¨å±€å”¯ä¸€çš„ Index å¯¹è±¡"""
    # 1. è¿æ¥å®¢æˆ·ç«¯
    # å»ºç«‹åŒå®¢æˆ·ç«¯ï¼šåŒæ­¥ç”¨äºæ™®é€šæ“ä½œï¼Œå¼‚æ­¥ç”¨äºé«˜å¹¶å‘æ£€ç´¢
    print("ğŸ”Œ è¿æ¥ Qdrant ...")
    client = qdrant_client.QdrantClient(url=settings.QDRANT_URL)
    aclient = qdrant_client.AsyncQdrantClient(url=settings.QDRANT_URL)

    # 2. å®šä¹‰å­˜å‚¨åç«¯
    vector_store = QdrantVectorStore(
        client=client,
        aclient=aclient,
        collection_name=settings.COLLECTION_NAME,
        enable_hybrid=True, # å¼€å¯æ··åˆæ£€ç´¢ (å…³é”®è¯+å‘é‡)
        # batch_size=20,    # å¦‚æœæŠ¥é”™å†…å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒå°è¿™ä¸ª
    )
    
    # 3. ç»„è£…ä¸Šä¸‹æ–‡
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    print("âœ… Qdrant è¿æ¥æˆåŠŸ")
    
    # 4. è¿”å› Index (æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä¼ å…¥ embed_modelï¼Œå¦åˆ™å®ƒä¼šå»ä¸‹ OpenAI çš„)
    return VectorStoreIndex.from_vector_store(
        vector_store=vector_store,
        embed_model=ModelFactory.get_embed_model() # è°ƒç”¨ä¸Šé¢çš„å·¥å‚
    )
