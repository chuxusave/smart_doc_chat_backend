# app/services/llm_factory.py
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from langchain_openai import ChatOpenAI
from app.core.config import get_settings
import torch

# ä½¿ç”¨ å•ä¾‹æ¨¡å¼ (Singleton) æˆ– lru_cache æ¥ç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¯·æ±‚éƒ½åŠ è½½ã€‚

settings = get_settings()

class ModelFactory:
    _embed_model = None
    _reranker = None
    _llm = None

    @classmethod
    def get_embed_model(cls):
        if cls._embed_model is None:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½ Embedding: {settings.EMBEDDING_MODEL_PATH} ...")
            cls._embed_model = HuggingFaceEmbedding(
                model_name=settings.EMBEDDING_MODEL_PATH,
                device="cuda" if torch.cuda.is_available() else "cpu", # æœ‰æ˜¾å¡ç”¨æ˜¾å¡ï¼Œæ²¡æ˜¾å¡ç”¨ CPU
                trust_remote_code=True # å…è®¸æ‰§è¡Œæ¨¡å‹é‡Œçš„è‡ªå®šä¹‰ Python ä»£ç 
            )
        return cls._embed_model

    @classmethod
    def get_reranker(cls):
        if cls._reranker is None:
            print("ğŸ”„ æ­£åœ¨åŠ è½½ Reranker ...")
            cls._reranker = FlagEmbeddingReranker(
                top_n=3, # æœ€ç»ˆåªé€‰å‡º 3 ä¸ªæœ€å¥½çš„ç»™å¤§æ¨¡å‹çœ‹ï¼Œè¿™èƒ½æå¤§å‡å°‘å¤§æ¨¡å‹çš„å¹»è§‰ï¼Œå¹¶èŠ‚çœ Token è´¹ç”¨ã€‚
                model=settings.RERANK_MODEL_PATH,
                use_fp16=False # æ˜¯å¦å¼€å¯åŠç²¾åº¦åŠ é€Ÿï¼ˆCPU å¿…é¡»å…³ï¼ŒGPU å¯ä»¥å¼€ä»¥çœæ˜¾å­˜ï¼‰
            )
        return cls._reranker

    @classmethod
    def get_llm(cls):
        if cls._llm is None:
            cls._llm = ChatOpenAI(
                openai_api_base=settings.DASHSCOPE_BASE_URL,
                openai_api_key=settings.DASHSCOPE_API_KEY,
                model="qwen-max", # é€šä¹‰åƒé—® Maxï¼ˆé˜¿é‡Œçš„æœ€å¼ºæ¨¡å‹ï¼‰
                temperature=0, # å¿…é¡»ä¸º 0ï¼Œä¿è¯å·¥å…·è°ƒç”¨ç¨³å®š
                streaming=True # æµå¼è¾“å‡ºï¼ˆåƒæ‰“å­—æœºä¸€æ ·ä¸€ä¸ªå­—ä¸€ä¸ªå­—è¹¦ï¼‰
            )
        return cls._llm