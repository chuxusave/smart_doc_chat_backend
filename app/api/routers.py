# app/api/routers.py
from fastapi import APIRouter, Depends, Header, UploadFile, File, BackgroundTasks, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Optional
from pydantic import BaseModel
import json
import qdrant_client
from app.core.config import get_settings

# --- Imports from App Structure ---
from app.utils.database import get_db
from app.core.redis import redis_manager
from app.services.llm_factory import ModelFactory
from app.services.file_service import handle_file_upload
from app.services.query_rewriter import condense_question
from app.core.models import Feedback  # ğŸ‘ˆ å‡è®¾ä½ ç§»åŠ¨äº† models.py
from app.core.prompts import DB_SCHEMA_TEXT, CORE_SYSTEM_PROMPT # ğŸ‘ˆ å‡è®¾ä½ ç§»åŠ¨äº† prompts.py

# --- LangChain & Langfuse ---
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langfuse import Langfuse
from langfuse.langchain import CallbackHandler

# --- Tools ---
from app.tools.policy_tool import lookup_policy_doc
from app.tools.sql_tool import query_business_data

import os
from dotenv import load_dotenv
load_dotenv()


router = APIRouter()
# langfuse = Langfuse()


# --- DTOs ---
class ChatRequest(BaseModel):
    message: str

class FeedbackRequest(BaseModel):
    session_id: str
    question: str
    answer: str
    rating: int
    tags: List[str] = []
    comment: Optional[str] = ""

# --- Helper: Get History ---
def get_chat_history_dep(x_session_id: str = Header(..., alias="X-Session-ID")):
    return redis_manager.get_chat_history(x_session_id)

settings = get_settings()
# ==========================
# 1. ğŸ’¬ Chat æ¥å£
# ==========================
@router.post("/chat")
async def chat_endpoint(
    request: ChatRequest,
    x_session_id: str = Header(..., alias="X-Session-ID"),
    history_dicts: List[dict] = Depends(get_chat_history_dep)
):
    print(f"ğŸ”” æ–°è¯·æ±‚ Session ID: {x_session_id}, å†å²æ¶ˆæ¯æ•°: {len(history_dicts)}")

  
    # 0. æŸ¥è¯¢æ”¹å†™ 
    # å°†æ”¹å†™åçš„é—®é¢˜ç”¨äº Agent æ¨ç†ï¼Œä½†å†å²è®°å½•ä¸­ä»ä¿å­˜ç”¨æˆ·åŸè¯
    final_query = condense_question(history_dicts, request.message)
    # 1. å‡†å¤‡å·¥å…·å’Œæ¨¡å‹
    tools = [lookup_policy_doc, query_business_data]
    llm = ModelFactory.get_llm()

    # 2. è½¬æ¢å†å²è®°å½• (Dict -> LangChain Objects)
    lc_history = []
    for msg in history_dicts:
        if msg.get("role") == "user":
            lc_history.append(HumanMessage(content=msg.get("content")))
        elif msg.get("role") == "assistant":
            lc_history.append(AIMessage(content=msg.get("content")))

    langfuse = Langfuse()
    # 3. åŠ¨æ€è·å– Prompt (CMS æ¨¡å¼)
    try:
        # cache_ttl_seconds=0 æ–¹ä¾¿è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒå¯å»æ‰
        langfuse_prompt = langfuse.get_prompt("rag-core-system", cache_ttl_seconds=0)
        final_system_prompt_str = langfuse_prompt.compile(schema=DB_SCHEMA_TEXT)
        print(f"âœ… Prompt æ‹‰å–æˆåŠŸ: {final_system_prompt_str}")
    except Exception as e:
        print(f"âš ï¸ Prompt æ‹‰å–å¤±è´¥: {e}")
        # å…œåº•é€»è¾‘
        final_system_prompt_str = CORE_SYSTEM_PROMPT.format()

    # 4. æ„å»º Agent
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=final_system_prompt_str),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])
    
    agent = create_tool_calling_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    
    # 5. å®šä¹‰æµå¼ç”Ÿæˆå™¨
    async def event_generator():
        full_response = ""
        captured_sources = [] # ğŸŸ¢  åˆå§‹åŒ–å®¹å™¨ï¼Œç”¨äºæš‚å­˜æ¥æºä¿¡æ¯
        langfuse_handler = CallbackHandler()
        
        try:
            async for event in agent_executor.astream_events(
                {"input": final_query, "chat_history": lc_history},
                version="v1",
                config={
                    "callbacks": [langfuse_handler],
                    "metadata": {
                        "langfuse_session_id": x_session_id,
                        "langfuse_user_id": "user_default"
                    }
                }
            ):
                kind = event["event"]
                # ğŸŸ¢ 2. ç›‘å¬å·¥å…·æ‰§è¡Œç»“æŸäº‹ä»¶
                if kind == "on_tool_end":
                    # æ‰“å°æ—¥å¿—æ–¹ä¾¿è°ƒè¯•
                    print(f"ğŸ”§ Tool End: {event['name']}")
                    
                    # ä»…å¤„ç†æ–‡æ¡£æ£€ç´¢å·¥å…·çš„ Source
                    if event["name"] == "lookup_policy_doc":
                        try:
                            tool_output_str = event["data"].get("output")
                            # ğŸ›¡ï¸ é˜²å¾¡æ€§ç¼–ç¨‹ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ä¸”åƒ JSON
                            if tool_output_str and isinstance(tool_output_str, str):
                                # å°è¯•æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown ä»£ç å—æ ‡è®° (```json ... ```)
                                clean_str = tool_output_str.strip()
                                if clean_str.startswith("```"):
                                    clean_str = clean_str.strip("`").replace("json", "").strip()
                                
                                # è§£æ JSON
                                output_json = json.loads(clean_str)
                                
                                if isinstance(output_json, dict) and "sources" in output_json:
                                    captured_sources = output_json["sources"]
                                    print(f"âœ… æ•è·åˆ° Sources: {len(captured_sources)} ä¸ª")
                            else:
                                print(f"âš ï¸ å·¥å…·è¾“å‡ºæ ¼å¼å¼‚å¸¸: {type(tool_output_str)}")
                                
                        except json.JSONDecodeError:
                            print(f"âš ï¸ å·¥å…·è¾“å‡ºä¸æ˜¯æœ‰æ•ˆçš„ JSON (å¯èƒ½æ˜¯æŠ¥é”™ä¿¡æ¯): {tool_output_str}")
                        except Exception as e:
                            print(f"âš ï¸ è§£æ Sources æœªçŸ¥é”™è¯¯: {e}")
                # 3. æ­£å¸¸çš„ LLM æµå¼è¾“å‡º        
                if kind == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    if chunk.content:
                        yield chunk.content
                        full_response += chunk.content
            # ğŸŸ¢ 4. åœ¨æµç»“æŸåï¼Œè¿½åŠ  Sources åè®®æ•°æ®
            # åªæœ‰å½“ç¡®å®æ£€ç´¢åˆ°äº†æ¥æºæ—¶æ‰å‘é€
            if captured_sources:
                # æŒ‰ç…§å‰ç«¯åè®®ï¼šæ¢è¡Œ + __SOURCES__ + æ¢è¡Œ + JSON
                sources_payload = json.dumps(captured_sources, ensure_ascii=False)
                yield f"\n\n__SOURCES__\n{sources_payload}"

            # 6. ä¿å­˜å†å²åˆ° Redis
            if full_response:
                new_history = history_dicts + [
                    {"role": "user", "content": request.message},
                    {"role": "assistant", "content": full_response}
                ]
                redis_manager.save_chat_history(x_session_id, new_history)
                
        except Exception as e:
            yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

    return StreamingResponse(event_generator(), media_type="text/plain")

# ==========================
# 2. ğŸ“¤ ä¸Šä¼ æ¥å£
# ==========================
@router.post("/upload")
async def upload_file(
    file: UploadFile = File(...), 
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    task_id = await handle_file_upload(file, background_tasks)
    return {"status": "success", "task_id": task_id, "message": "å¼€å§‹åå°å¤„ç†"}

@router.get("/upload/{task_id}")
async def get_upload_status(task_id: str):
    task_info = redis_manager.get_client().hgetall(f"task:{task_id}")
    if not task_info:
        return JSONResponse(status_code=404, content={"status": "not_found"})
    return task_info

# ==========================
# 3. â­ åé¦ˆæ¥å£
# ==========================
@router.post("/feedback")
async def submit_feedback(
    request: FeedbackRequest,
    db: AsyncSession = Depends(get_db)
):
    try:
        tags_str = ",".join(request.tags)
        new_feedback = Feedback(
            session_id=request.session_id,
            question=request.question,
            answer=request.answer,
            rating=request.rating,
            tags=tags_str,
            comment=request.comment
        )
        db.add(new_feedback)
        await db.commit()
        await db.refresh(new_feedback)
        return {"status": "success", "id": new_feedback.id}
    except Exception as e:
        await db.rollback()
        return JSONResponse(status_code=500, content={"detail": str(e)})
    

# ==========================
# 4. ğŸ“‚ æ–‡ä»¶åˆ—è¡¨æ¥å£ (è¡¥å…¨è¿™ä¸ª)
# ==========================
@router.get("/files")
async def get_indexed_files():
    """è·å–çŸ¥è¯†åº“ä¸­å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        # è¿æ¥ Qdrant
        client = qdrant_client.QdrantClient(url=settings.QDRANT_URL)
        
        # 1. æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        if not client.collection_exists(settings.COLLECTION_NAME):
            return {"count": 0, "files": []}

        # 2. éå†æ•°æ® (è¿™é‡Œç®€å•å–å‰100ä¸ªç”¨äºå±•ç¤º)
        # ç”Ÿäº§ç¯å¢ƒå¦‚æœæ–‡ä»¶å¾ˆå¤šï¼Œå¯ä»¥ä½¿ç”¨ Scroll åˆ†é¡µ
        scroll_result = client.scroll(
            collection_name=settings.COLLECTION_NAME,
            limit=100,
            with_payload=True,
            with_vectors=False
        )
        
        points, _ = scroll_result
        files = set()
        for point in points:
            # æå– payload é‡Œçš„ file_name
            if point.payload and "file_name" in point.payload:
                files.add(point.payload["file_name"])
        
        return {"count": len(files), "files": list(files)}
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        # å‡ºé”™ä¸è¿”å› 500ï¼Œè¿”å›ç©ºåˆ—è¡¨é˜²æ­¢å‰ç«¯å´©
        return {"count": 0, "files": []}    